{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Media Rating by Retweet and Quote Count\n",
    "\n",
    "This notebook is meant to follow [Evaluating Content](./Evaluating_Content.ipynb). The minimal requirements for this notebook are met by utilizing [this notebook](./Evaluating_Content.ipynb) first. \n",
    "\n",
    "The SQLite3 database created in that notebook will be accessed using the Pandas library as an alternative means of accessing the data.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Create big data visualizations using Pandas, Seaborn and Matplotlib packages. Interact with data from an SQLite3 database using Pandas.\n",
    "\n",
    "### Learning Goals\n",
    "- Use Pandas to extract SQLite3 database data.\n",
    "- Become familiar with Pandas Dataframes.\n",
    "- Utilize Seaborn package to create visualizations.\n",
    "- Recognize different types of graphs that can be used to represent multivariate datasets.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* Visit [Evaluating Content](./Evaluating_Content.ipynb) to prepare the environment for this notebook.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the environment\n",
    "- Load Packages\n",
    "- Create a copy of the original database\n",
    "- Open a connection."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load Packages\n",
    "\n",
    "# Enable Matplotlib Juupyter Widget Backend\n",
    "%matplotlib widget\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from shutil import copyfile"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Copy Database\n",
    "DB_FILE = \"./pandas_tweets.db\"\n",
    "copyfile(\"tweets.db\", DB_FILE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Connect to database copy\n",
    "connection = sqlite3.connect(DB_FILE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Access Database Query**\n",
    "\n",
    "Pandas `pd` includes a method called `pd.read_sql_query` that given an SQL query and a database connection will generate a Dataframe.\n",
    "\n",
    "### **The `sqlite_master` Table**\n",
    "\n",
    "Lets try it getting the table names from the database copy using the `connection` and this query:\n",
    "```\n",
    "SELECT name, sql \n",
    "FROM sqlite_master \n",
    "WHERE type='table';\n",
    "```\n",
    "\n",
    "The table `sqlite_master` is part of the SQLite3 structure and can be used to get information about the structure of the database. This query in particular retrieves the tables and the commands used to create them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tables = pd.read_sql_query(\n",
    "    \"\"\"SELECT name, sql\n",
    "    FROM sqlite_master \n",
    "    WHERE type='table';\"\"\",\n",
    "    connection\n",
    ")\n",
    "tables.head(15)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Retrieve a comple table**\n",
    "A complete table can be retrieved by using a simmilar method `pd.read_sql_table` that takes a table name and the connection."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_detail = pd.read_sql_query(\"SELECT * FROM tweet_auto_detail;\", connection)\n",
    "\n",
    "auto_detail.loc[ auto_detail.has_media == 0, \"has_media_label\"] = \"Media\"\n",
    "auto_detail.loc[ auto_detail.has_media == 1, \"has_media_label\"] = \"No Media\"\n",
    "\n",
    "auto_detail.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Describing the data\n",
    "\n",
    "## Date range\n",
    "\n",
    "The `DatePublished` column holds timestamps of when the data was published. This format does not make it easy to know the actual dates."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_detail[\"datePublished\"].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transform timestamps to dates \n",
    "\n",
    "A transformation is required to change this `float64` values to date and time. The function `pd.Timestamp` accepts float value timestamps and transforms them into readable dates. The method `apply` allows performing a transformation to values in a `dataframe`, this transformations most accept a single input and return a single output.\n",
    "\n",
    "The function `timestamp2DateTimeBySegment` will be our transformation in this occasion."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def timestamp2DateTimeBySegment(ts: float):\n",
    "    \"\"\"Transformation from float to DateTime by segment_size\"\"\"\n",
    "    segment_size = 3600.0*24\n",
    "    minus4_TZ = 3600.0*-4.0\n",
    "    return pd.Timestamp(int((ts)/segment_size)*segment_size, unit='s', tz='UTC')\n",
    "\n",
    "# auto_detail[\"datePublished_DT\"] = auto_detail[\"datePublished\"].apply(lambda x: pd.Timestamp(int((x+minus4_TZ)/seconds_in_hour)*seconds_in_hour, unit='s'))\n",
    "auto_detail[\"datePublished_DT\"] = auto_detail[\"datePublished\"].apply(timestamp2DateTimeBySegment)\n",
    "print(\"Original Float Timestamps:\")\n",
    "print(auto_detail[\"datePublished\"].describe(datetime_is_numeric=True))\n",
    "print(\"\\nTransformed into DateTime by the hour:\")\n",
    "print(auto_detail[\"datePublished_DT\"].describe(datetime_is_numeric=True))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 1\n",
    "\n",
    "What is the earliest date of a tweet captured in this data set?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Answer here with code\n",
    "\n",
    "#Example latest date would be:\n",
    "print(auto_detail[\"datePublished_DT\"].max())\n",
    "\n",
    "# Response:\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing Time data\n",
    "\n",
    "**Histograms** are a great tool to visualize frequency over any one variable.\n",
    "\n",
    "The code bellow produces a histogram of the `datePublished_DT`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a Mask to limit date range\n",
    "start_date = \"2019-07-11\"\n",
    "end_date = \"2019-08-01\"\n",
    "mask=(auto_detail[\"datePublished_DT\"] >= start_date) & (auto_detail[\"datePublished_DT\"] < end_date)\n",
    "\n",
    "# Take sample\n",
    "sample_dates = auto_detail[mask]\n",
    "\n",
    "#Generate histogram bin limits with numpy\n",
    "edges=np.histogram_bin_edges(sample_dates[\"datePublished\"])\n",
    "edges=pd.DataFrame(edges, columns=(\"datePublished\",))\n",
    "edges[\"datePublished_DT\"] = edges[\"datePublished\"].apply(timestamp2DateTimeBySegment)\n",
    "\n",
    "# Visualize a histogram\n",
    "sample_dates.hist(column=\"datePublished_DT\", xrot=25, bins=edges[\"datePublished_DT\"], figsize=(7,7), backend=\"matplotlib\")\n",
    "plt.title(\"Histogram: Date Published\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Date\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frequency by Language and Date\n",
    "\n",
    "It is posible to generate multiple histograms for different groups of the data. In particular we can see a time distribution of number of tweets over time for each language.\n",
    "\n",
    "In this occasion the `column` is set to `datePublished_DTdatePublished_DT` and we also use the `by` aparameter to set the grouping column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_detail.hist(column='datePublished_DT', by='language', bins=45, figsize=(8,9), sharex=True, sharey=True);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the histograms above it is very clear that most activity was in Spanish followed by English. It is one of the reasons the team integrated the work with the Google Translation API for the visualizations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate a table of frequencies\n",
    "\n",
    "The column language is currently of type object as this values are separate strings. Pandas offers a different data type that reduces memory usage called `Category`. `Categories` are particularly useful when a few values will be repeated many times."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_detail.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_detail['language']=auto_detail['language'].astype('category')\n",
    "print(auto_detail['language'].describe())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can verify that the datatype has changed from `object` to `category`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auto_detail[['language']].dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Group By & Count\n",
    "To get the **totals** per language we can use a group by statement with a count operation. as shown bellow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_by_lang = auto_detail[[\"language\", \"tweet_id\"]].groupby(\n",
    "    [\"language\"]\n",
    ").count()\n",
    "count_by_lang=count_by_lang.rename(\n",
    "    columns = {\"tweet_id\":\"Total\"}, inplace = False)\n",
    "count_by_lang.transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most user activity appears to be in Spanish followed by English."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2\n",
    "\n",
    "A. How many tweets have multimedia?\n",
    "A. Transform the `has_media` column into a categorical column to enhance efficiency:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Using Group By & Count\n",
    "# Display how many tweets have media and how many don't\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transform the column here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print dtype of the columns to display the updated format\n",
    "auto_detail.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ranking Tweets with Multimedia by User Interaction \n",
    "\n",
    "Tweets can be ranked by two readily available metrics `Retweet Count` and `Favorite Count`. Other metrics such as `Comment Count` and `Quote Count` require access to API other than the standard API. However they could be used in a similar way to the ones shown bellow.\n",
    "\n",
    "We are particularly intrested in discovering relevant multimedia shared through social media. This will require filtering our dataset to only the tweets with multimedia. \n",
    "\n",
    "The easiest way to filter a dataframe on a categorical column would be to use the [`.loc` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html). Using this method it is possible to select only the rows that comply with a specific condition.\n",
    "\n",
    "The `sort_values` method allows arranging the dataframe in descending or ascending order, read more on the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html).\n",
    "\n",
    "Using this `pandas` methods allows us to quickly identify popular tweets with multimedia."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get Top 20 Tweets with Media by Retweet Count\n",
    "from tweet_requester.display import prettyPrintDataFrame\n",
    "\n",
    "\n",
    "# page skips to the next N\n",
    "page=1\n",
    "N = 20\n",
    "\n",
    "top_N_retweet = auto_detail.loc[auto_detail[\"has_media\"]==1].sort_values(\"retweetCount\", ascending=False).head(N*page).tail(N)[[\"retweetCount\", \"tweet_id\", \"url\"]]\n",
    "prettyPrintDataFrame(top_N_retweet, max_column=60)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get Top 20 Tweets with Media by Favorite(❤️) Count\n",
    "\n",
    "# page skips to the next N\n",
    "page=1\n",
    "N = 20\n",
    "\n",
    "top_N_favorite = auto_detail.loc[auto_detail[\"has_media\"]==1].sort_values(\"favoriteCount\", ascending=False).head(N*page).tail(N)[[\"favoriteCount\", \"tweet_id\", \"url\"]]\n",
    "prettyPrintDataFrame(top_N_favorite, max_column=60)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Graph Date vs\n",
    "# auto_detail.datePublished.apply(lambda dt: int(str(dt.year)+ str(dt.month) + str(dt.day) +str(dt.hour) + str(dt.minute)))\n",
    "# auto_detail.datePublished.apply(lambda dt: \"{:02}{:02}{:02}\".format(dt.day, dt.hour, dt.minute))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi Variable Relations\n",
    "\n",
    "It is possible to create multivariate relationship visualizations both in 2D and 3D format using the `matplolib` library.\n",
    "\n",
    "### 3D Scatter Plot\n",
    "\n",
    "This plot displays both the correlation between retweets and favorites, but also displays how most of the shares were concentrated in time.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig4 = plt.figure(figsize=(8,6))\n",
    "ax = Axes3D(fig4, auto_add_to_figure=False)\n",
    "fig4.add_axes(ax)\n",
    "\n",
    "# get colormap from seaborn\n",
    "cmap = ListedColormap(sns.color_palette(\"husl\", 2).as_hex())\n",
    "dates = auto_detail[\"datePublished_DT\"]\n",
    "\n",
    "sc = ax.scatter(\n",
    "    auto_detail[\"retweetCount\"], # X\n",
    "    auto_detail[\"favoriteCount\"], # Y\n",
    "    auto_detail[\"datePublished\"], # Z\n",
    "    s=40,\n",
    "    c=auto_detail[[\"has_media\"]],\n",
    "    cmap=cmap,\n",
    "    alpha=1,\n",
    "    marker=\"o\"\n",
    ")\n",
    "ax.set_xlabel(\"Retweet Count\")\n",
    "ax.set_ylabel(\"Favorite Count\")\n",
    "ax.set_zlabel(\"Date Published\")\n",
    "# ax.zaxis.set_major_formatter(mdates.DayLocator(interval=1))\n",
    "\n",
    "# Legend\n",
    "plt.legend(*sc.legend_elements(), bbox_to_anchor=(1., 1), loc=2)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FacetGrid\n",
    "\n",
    "The `FacetGrid` is particularly useful to demonstrate different patterns for in different categories.\n",
    "\n",
    "The grid of graphs bellow displays how languages other than Spanish and English didn't show as much user activity in terms of retweets and favorites."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "groups = sns.FacetGrid(auto_detail, col=\"has_media\", row=\"language\", hue=\"datePublished_DT\", legend_out=False)\n",
    "groups.map(sns.scatterplot, \"favoriteCount\", \"retweetCount\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "The notebooks of [Evaluating Content](./3-Evaluating_Content.ipynb) and this notebook, Media_Rating, offer an example of how multiple notebooks can be used in a Curatorial and Analytical environment. In particualr the notebooks demonstrate that custom web interfaces can be developed inside Jupyter Notebooks to process the data in different stages and generate consistent reports.\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}